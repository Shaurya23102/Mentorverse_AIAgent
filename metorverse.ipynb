{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f737a079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: C:\\Users\\Shaur\\OneDrive\\Desktop\\docs\\college\\faculty_data\\iiit-bhubneswar.csv\n",
      "Loading file: C:\\Users\\Shaur\\OneDrive\\Desktop\\docs\\college\\faculty_data\\iiitbanglore.csv\n",
      "Loading file: C:\\Users\\Shaur\\OneDrive\\Desktop\\docs\\college\\faculty_data\\iiitnr_faculty.csv\n",
      "Loading file: C:\\Users\\Shaur\\OneDrive\\Desktop\\docs\\college\\faculty_data\\iitb.csv\n",
      "Loading file: C:\\Users\\Shaur\\OneDrive\\Desktop\\docs\\college\\faculty_data\\iiti.csv\n",
      "Loading file: C:\\Users\\Shaur\\OneDrive\\Desktop\\docs\\college\\faculty_data\\iitj.csv\n",
      "Loading file: C:\\Users\\Shaur\\OneDrive\\Desktop\\docs\\college\\faculty_data\\iitm.csv\n",
      "\n",
      "--- Summary ---\n",
      "Number of CSV files loaded: 7\n",
      "Columns in first CSV: ['data-page-selector', 'professor', 'position', 'email', 'department', 'contact', 'Research_Interests_0', 'Education_-_PhD_0', 'Education_-_MTech_0', 'Education_-_BTech_0']\n",
      "\n",
      "First few rows of first CSV:\n",
      "                                   data-page-selector  \\\n",
      "0  https://www.iiit-bh.ac.in/faculty/satyajit-pan...   \n",
      "1  https://www.iiit-bh.ac.in/faculty/dr-ravina-sh...   \n",
      "2  https://www.iiit-bh.ac.in/faculty/dr-prajnyaji...   \n",
      "3  https://www.iiit-bh.ac.in/faculty/dr-santisudh...   \n",
      "4  https://www.iiit-bh.ac.in/faculty/prof-meenu-r...   \n",
      "\n",
      "                  professor             position  \\\n",
      "0        Satyajit Panigrahy  Assistant Professor   \n",
      "1         Dr. Ravina Sharma  Assistant Professor   \n",
      "2    Dr. Prajnyajit Mohanty  Assistant Professor   \n",
      "3  Dr. Santisudha Panigrahi  Assistant Professor   \n",
      "4      Prof. Meenu Rani Dey  Assistant Professor   \n",
      "\n",
      "                                email                             department  \\\n",
      "0    Email Id: satyajit@iiit-bh.ac.in   Department of Electrical Engineering   \n",
      "1      Email Id: ravina@iiit-bh.ac.in                                    NaN   \n",
      "2  Email Id: prajnyajit@iiit-bh.ac.in  Department of Electronics Engineering   \n",
      "3  Email Id: santisudha@iiit-bh.ac.in         Department of Computer Science   \n",
      "4   Email Id: meenurani@iiit-bh.ac.in         Department of Computer Science   \n",
      "\n",
      "                   contact                               Research_Interests_0  \\\n",
      "0  Contact no.: 8093338893  Prof. Satyajit Panigrahy's teaching interests ...   \n",
      "1                      NaN  Dr.Ravina's teaching and research interests in...   \n",
      "2  Contact no.: 9040452405  Prof. Prajnyajit Mohantyâ€™s teaching interests ...   \n",
      "3  Contact no.: 7387094590  Dr. Santisudha's Teaching interests are AI/ML,...   \n",
      "4                      NaN  Prof. Meenu's teaching and research interests ...   \n",
      "\n",
      "                                   Education_-_PhD_0  \\\n",
      "0  Ph.D. - National Institute of Technology Rourk...   \n",
      "1                    Ph.D., Banaras Hindu University   \n",
      "2  Ph.D. - National Institute of Technology Rourk...   \n",
      "3                            PhD in Computer Science   \n",
      "4  PhD, Indian Institute of Technology Guwahati, ...   \n",
      "\n",
      "                                 Education_-_MTech_0  \\\n",
      "0  M.Tech - National Institute of Technology Hami...   \n",
      "1                                                NaN   \n",
      "2  M.Tech - Veer Surendra Sai University of Techn...   \n",
      "3                                              Mtech   \n",
      "4                            MTech, IIIT Bhubaneswar   \n",
      "\n",
      "                                 Education_-_BTech_0  \n",
      "0  B.Tech - Vignan Institute of Technology And Ma...  \n",
      "1                                                NaN  \n",
      "2  B.Tech - National Institute of Science and Tec...  \n",
      "3                                                 BE  \n",
      "4                            BE, CSVTU, Chhattisgarh  \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = r\"C:\\Users\\Shaur\\OneDrive\\Desktop\\docs\\college\\faculty_data\"\n",
    "# Find all CSV files in the directory\n",
    "csv_files = glob.glob(os.path.join(directory_path, \"*.csv\"))\n",
    "\n",
    "# List to hold all DataFrames from all CSVs\n",
    "all_dataframes = []\n",
    "\n",
    "# Loop through each CSV file and load it\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Loading file: {csv_file}\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    all_dataframes.append(df)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Number of CSV files loaded: {len(all_dataframes)}\")\n",
    "\n",
    "if all_dataframes:\n",
    "    print(\"Columns in first CSV:\", list(all_dataframes[0].columns))\n",
    "    print(\"\\nFirst few rows of first CSV:\\n\", all_dataframes[0].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6ea7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents created: 315\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "import re, os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = r\"C:\\Users\\Shaur\\OneDrive\\Desktop\\docs\\college\\faculty_data\"\n",
    "documents = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        csv_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        first_col_name = df.columns[0]\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            # 1. Extract the first column's value for Metadat\n",
    "            first_col_value = row[first_col_name]\n",
    "            other_cols = df.columns[1:]\n",
    "            row_text = \", \".join([f\"{col}: {row[col]}\" for col in other_cols])\n",
    "            row_text = re.sub(r\"\\s+\", \" \", row_text).strip()\n",
    "\n",
    "            doc = Document(\n",
    "                page_content=row_text,\n",
    "                metadata={\n",
    "                    \"row_index\": idx,\n",
    "                    \"source\": file,\n",
    "                    first_col_name: first_col_value  # Specifically storing the first column here\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "print(f\"Total documents created: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285375dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 315\n",
      "Average Character Length: 545.65\n",
      "Average Word Count: 68.37\n",
      "Max Word Count: 380\n",
      "Min Word Count: 5\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate lengths\n",
    "char_lengths = [len(doc.page_content) for doc in documents]\n",
    "word_lengths = [len(doc.page_content.split()) for doc in documents]\n",
    "\n",
    "# 2. Compute Averages\n",
    "if documents:\n",
    "    avg_char = sum(char_lengths) / len(documents)\n",
    "    avg_word = sum(word_lengths) / len(documents)\n",
    "\n",
    "    print(f\"Total Chunks: {len(documents)}\")\n",
    "    print(f\"Average Character Length: {avg_char:.2f}\")\n",
    "    print(f\"Average Word Count: {avg_word:.2f}\")\n",
    "    print(f\"Max Word Count: {max(word_lengths)}\")\n",
    "    print(f\"Min Word Count: {min(word_lengths)}\")\n",
    "else:\n",
    "    print(\"No documents found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e2e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n",
      "{'row_index': 3, 'source': 'iiit-bhubneswar.csv', 'data-page-selector': 'https://www.iiit-bh.ac.in/faculty/dr-santisudha-panigrahi/'}\n",
      "professor: Dr. Santisudha Panigrahi, position: Assistant Professor, email: Email Id: santisudha@iiit-bh.ac.in, department: Department of Computer Science, contact: Contact no.: 7387094590, Research_Interests_0: Dr. Santisudha's Teaching interests are AI/ML,Python, Java, C++ Programming Languages, Software Engineering, and Research interests include AI/ML, Bioinformatics, Image Processing, Computer Vision, Education_-_PhD_0: PhD in Computer Science, Education_-_MTech_0: Mtech, Education_-_BTech_0: BE\n"
     ]
    }
   ],
   "source": [
    "a = documents[3].metadata\n",
    "b = documents[3].page_content\n",
    "print(len(b))\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a752ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIAGENTS\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Shaur\\AppData\\Local\\Temp\\ipykernel_12332\\2193803200.py:19: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Processing 315 records into Multi-Vector store...\n",
      "âœ… Indexing Complete and embeddings persisted to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaur\\AppData\\Local\\Temp\\ipykernel_12332\\2193803200.py:63: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  retriever.vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_classic.retrievers import MultiVectorRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.stores import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# ================= PATH =================\n",
    "CHROMA_PATH = r\"D:\\AIAGENTS\\chromadb_\"\n",
    "os.makedirs(CHROMA_PATH, exist_ok=True)\n",
    "\n",
    "# 1. Initialize the Embedding Model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# 2. Setup the Storage Layers (PERSISTENT)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"faculty_data\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=CHROMA_PATH\n",
    ")\n",
    "\n",
    "# Docstore holds the full parent documents (still in-memory)\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# 3. Initialize the Retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# ðŸ”¹ IMPORTANT: fetch more child chunks to ensure 10 parents\n",
    "retriever.search_kwargs = {\"k\": 50}\n",
    "\n",
    "def index_data(retriever, documents):\n",
    "    print(f\"ðŸš€ Processing {len(documents)} records into Multi-Vector store...\")\n",
    "\n",
    "    for parent_doc in documents:\n",
    "        parent_id = str(uuid.uuid4())\n",
    "\n",
    "        column_pairs = parent_doc.page_content.split(\", \")\n",
    "\n",
    "        child_docs = []\n",
    "        for pair in column_pairs:\n",
    "            child_docs.append(\n",
    "                Document(\n",
    "                    page_content=pair,\n",
    "                    metadata={id_key: parent_id}\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Store full parent document\n",
    "        retriever.docstore.mset([(parent_id, parent_doc)])\n",
    "\n",
    "        # Store child embeddings\n",
    "        retriever.vectorstore.add_documents(child_docs)\n",
    "\n",
    "    # Persist embeddings\n",
    "    retriever.vectorstore.persist()\n",
    "\n",
    "    print(\"âœ… Indexing Complete and embeddings persisted to disk.\")\n",
    "\n",
    "# Run indexing\n",
    "index_data(retriever, documents)\n",
    "\n",
    "# ================= RETRIEVAL =================\n",
    "\n",
    "def retrieve_top_10(retriever, query: str):\n",
    "    \"\"\"\n",
    "    Returns top 10 FULL parent documents using MultiVectorRetriever\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(query)   # NEW API\n",
    "    return docs[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e056d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "# Initialize FlashRank model\n",
    "reranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "def retrieve_top_5_with_flashrank(retriever, query: str):\n",
    "    \"\"\"\n",
    "    Uses MultiVectorRetriever + FlashRank\n",
    "    Returns top 5 reranked parent documents\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Get parent docs from MultiVectorRetriever\n",
    "    parent_docs = retriever.invoke(query)\n",
    "\n",
    "    # Safety: if fewer docs\n",
    "    if len(parent_docs) <= 5:\n",
    "        return parent_docs\n",
    "\n",
    "    # Step 2: Prepare rerank request\n",
    "    passages = [\n",
    "        {\"id\": i, \"text\": doc.page_content}\n",
    "        for i, doc in enumerate(parent_docs)\n",
    "    ]\n",
    "\n",
    "    rerank_request = RerankRequest(\n",
    "        query=query,\n",
    "        passages=passages\n",
    "    )\n",
    "\n",
    "    # Step 3: Rerank\n",
    "    reranked = reranker.rerank(rerank_request)\n",
    "\n",
    "    # Step 4: Select top 5 documents\n",
    "    top_5_docs = []\n",
    "    for item in reranked[:5]:\n",
    "        top_5_docs.append(parent_docs[item[\"id\"]])\n",
    "\n",
    "    return top_5_docs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbd815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "designation: nan, email: aruna@iiitnr.edu.in, profile_link: https://www.iiitnr.ac.in/node/2481, raw_text_snippet: Faculty (Arranged Alphabetically) Abhishek Sharma ( ASSISTANT PROFESSOR, ECE ) Qualification : Ph. D. (IIT Guwahati) Department : ECE Email Id : abhishek[at]iiitnr.edu.in Contact No : 91-771-2474031 Research : Bio-metrics, Online Signature Verification, Write identification, Image Processing, Patter\n",
      "designation: nan, email: aruna@iiitnr.edu.in, profile_link: https://www.iiitnr.ac.in/node/2481, raw_text_snippet: Abhishek Sharma ( ASSISTANT PROFESSOR, ECE ) Qualification : Ph. D. (IIT Guwahati) Department : ECE Email Id : abhishek[at]iiitnr.edu.in Contact No : 91-771-2474031 Research : Bio-metrics, Online Signature Verification, Write identification, Image Processing, Pattern Recognition and Machine Learning\n",
      "designation: nan, email: vinay@iiitnr.edu.in, profile_link: https://www.iiitnr.ac.in/node/3301, raw_text_snippet: Vinay Kumar ( Assistant Professor, CSE ) Qualification : Ph.D. (CSE, IIT (BHU) Varanasi), Department : CSE Email Id : vinay@iiitnr.edu.in Research : Software Engineering, Reliability, Safety and Security Analysis of Computer based Systems, Mathematical Modeling, Machine Learning, Distributed Systems\n",
      "designation: nan, email: sachidanand@iiitnr.edu.in, profile_link: https://www.iiitnr.ac.in/node/3507, raw_text_snippet: Sachchida Nand Mishra ( (Assistant Professor (Contract), DSAI) ) Qualification : Ph.D. (NIT Durgapur) Email Id : sachidanand@iiitnr.edu.in Research : Fuzzy Graphs and Networks, Graph Theory, Operations Research, Machine Learning, Deep Learning, Data Analysis, Data Science, Computational Statistics.\n",
      "designation: nan, email: shashit@iiitnr.edu.in, profile_link: https://www.iiitnr.ac.in/node/3501, raw_text_snippet: Shashi Tiwari ( Assistant Professor (Contract) ECE ) Qualification : Ph. D. IIT (BHU) Varanasi Department : ECE Email Id : shashit@iiitnr.edu.in Research : â€¢ Micro- and Nano-electronic devices, Organic electronics and Sensors. â€¢ Fabrication and characterization of micro-and nano-electronic devices\n"
     ]
    }
   ],
   "source": [
    "query = \"faculty in iiitnr\"\n",
    "\n",
    "top_5_docs = retrieve_top_5_with_flashrank(retriever, query)\n",
    "\n",
    "for i, doc in enumerate(top_5_docs, 1):\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a9e978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.tools import tool\n",
    "from langchain_core.documents import Document\n",
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "# Initialize FlashRank once\n",
    "reranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "@tool(\"flashrank\")\n",
    "def flashrank(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Retrieve documents using MultiVectorRetriever\n",
    "    and rerank them using FlashRank.\n",
    "    Returns top 5 documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Retrieve parent documents\n",
    "    parent_docs = retriever.invoke(query)\n",
    "\n",
    "    if not parent_docs:\n",
    "        return []\n",
    "\n",
    "    if len(parent_docs) <= 5:\n",
    "        return parent_docs\n",
    "\n",
    "    # Step 2: Prepare passages\n",
    "    passages = [\n",
    "        {\"id\": i, \"text\": doc.page_content}\n",
    "        for i, doc in enumerate(parent_docs)\n",
    "    ]\n",
    "\n",
    "    rerank_request = RerankRequest(\n",
    "        query=query,\n",
    "        passages=passages\n",
    "    )\n",
    "\n",
    "    # Step 3: Rerank\n",
    "    reranked = reranker.rerank(rerank_request)\n",
    "\n",
    "    for rank, item in enumerate(reranked[:5]):\n",
    "        doc = parent_docs[item[\"id\"]]\n",
    "        doc.metadata[\"rerank_rank\"] = rank + 1\n",
    "        doc.metadata[\"source\"] = \"vectorstore\"\n",
    "        top_5_docs.append(doc)\n",
    "\n",
    "\n",
    "    return top_5_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9dc00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Initialize Tavily client\n",
    "tavily_client = TavilyClient(\n",
    "    api_key=\"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\"\n",
    ")\n",
    "def tavily_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the web using Tavily and return summarized results.\n",
    "    \"\"\"\n",
    "    response = tavily_client.search(\n",
    "        query=query,\n",
    "        search_depth=\"advanced\",\n",
    "        max_results=5\n",
    "    )\n",
    "\n",
    "    # Convert to readable text for LLMs\n",
    "    results = []\n",
    "    for r in response.get(\"results\", []):\n",
    "        results.append(\n",
    "            f\"Title: {r.get('title')}\\n\"\n",
    "            f\"Content: {r.get('content')}\\n\"\n",
    "            f\"URL: {r.get('url')}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n---\\n\".join(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90ff8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Initialize Wikipedia tool (once, globally)\n",
    "wiki = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(\n",
    "        top_k_results=2,\n",
    "        doc_content_chars_max=1000\n",
    "    )\n",
    ")\n",
    "\n",
    "def wikipedia_node(state):\n",
    "    \"\"\"\n",
    "    LangGraph node for Wikipedia lookup.\n",
    "    Uses Wikipedia transiently and does NOT store results in state.\n",
    "    \"\"\"\n",
    "    print(\"---WIKIPEDIA LOOKUP---\")\n",
    "\n",
    "    # Get latest query\n",
    "    query = state[\"messages\"][-1].content\n",
    "\n",
    "    # Call Wikipedia\n",
    "    result = wiki.invoke(query)\n",
    "\n",
    "    if not result:\n",
    "        return {}\n",
    "\n",
    "    # Inject temporary context into messages ONLY\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=f\"\"\"\n",
    "Wikipedia context (use only if relevant):\n",
    "\n",
    "{result}\n",
    "\n",
    "User question:\n",
    "{query}\n",
    "\"\"\"\n",
    "            )\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d56a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from typing import TypedDict, List, Dict\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    intent: str\n",
    "    memory: List[Dict[str, str]]\n",
    "    retrieved_docs: List[Document]\n",
    "    relevance_score: float\n",
    "    iteration_count: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e23ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_analyzer(state):\n",
    "    \"\"\"\n",
    "    Analyze the user query and normalize it using Gemini.\n",
    "    \"\"\"\n",
    "    print(\"---ANALYZE QUERY---\")\n",
    "    question = state[\"messages\"][0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Analyze the following query and clean it if needed.\n",
    "\n",
    "{question}\n",
    "\n",
    "Return a concise, clear research-oriented question.\n",
    "\"\"\")\n",
    "    ]\n",
    "\n",
    "\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=os.getenv(\"GEMINI\"),\n",
    "        temperature=0\n",
    ")\n",
    "\n",
    "    \n",
    "\n",
    "    response = model.invoke(msg)\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"iteration_count\": 0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7de86141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_check(state):\n",
    "    \"\"\"\n",
    "    Check conversation memory and retrieve relevant past information\n",
    "    for follow-up queries.\n",
    "    \"\"\"\n",
    "    print(\"---CHECK MEMORY---\")\n",
    "\n",
    "    question = state[\"messages\"][0].content\n",
    "    memory = state.get(\"memory\", [])\n",
    "\n",
    "    # No memory â†’ no update\n",
    "    if not memory:\n",
    "        return {}\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\n",
    "You are given a follow-up question and a list of past interactions.\n",
    "\n",
    "Follow-up question:\n",
    "{question}\n",
    "\n",
    "Conversation memory:\n",
    "{memory}\n",
    "\n",
    "From the memory, extract ONLY the information that is directly\n",
    "relevant to answering the follow-up question.\n",
    "\n",
    "If nothing is relevant, return: NONE\n",
    "Otherwise, return the relevant information concisely.\n",
    "\"\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=os.getenv(\"GEMINI\"),\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    response = model.invoke(msg)\n",
    "\n",
    "    if response.content.strip().upper() == \"NONE\":\n",
    "        return {}\n",
    "\n",
    "    # Return PARTIAL update only\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=f\"\"\"\n",
    "Context from previous interactions:\n",
    "{response.content}\n",
    "\n",
    "Current question:\n",
    "{question}\n",
    "\"\"\"\n",
    "            )\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6299a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_classifier(state):\n",
    "    \"\"\"\n",
    "    Classify the intent of the query using Gemini.\n",
    "    \"\"\"\n",
    "    print(\"---CLASSIFY INTENT---\")\n",
    "\n",
    "    question = state[\"messages\"][0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\n",
    "Classify the intent of the following query as one of:\n",
    "- new_search\n",
    "- refinement\n",
    "- follow_up\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Return ONLY one label.\n",
    "\"\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=os.getenv(\"GEMINI\"),\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    response = model.invoke(msg)\n",
    "\n",
    "    intent = response.content.strip().lower()\n",
    "\n",
    "    # Guardrail: enforce valid labels\n",
    "    if intent not in {\"new_search\", \"refinement\", \"follow_up\"}:\n",
    "        intent = \"new_search\"\n",
    "\n",
    "    # Return PARTIAL state update (do NOT return state)\n",
    "    return {\n",
    "        \"intent\": intent\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cc2d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_router(state):\n",
    "    \"\"\"\n",
    "    Route execution based on classified intent.\n",
    "    \"\"\"\n",
    "    intent = state.get(\"intent\", \"new_search\")\n",
    "\n",
    "    if intent == \"new_search\":\n",
    "        return \"new_search\"\n",
    "    elif intent == \"refinement\":\n",
    "        return \"refinement\"\n",
    "    else:\n",
    "        return \"follow_up\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "186d8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_flashrank(state):\n",
    "    \"\"\"\n",
    "    LangGraph node: calls FlashRank tool and updates state\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE WITH FLASHRANK---\")\n",
    "\n",
    "    query = state[\"messages\"][0].content\n",
    "\n",
    "    docs = flashrank.invoke(query)\n",
    "\n",
    "    if not docs:\n",
    "        return {}\n",
    "\n",
    "    return {\n",
    "        \"retrieved_docs\": docs,\n",
    "        \"iteration_count\": state[\"iteration_count\"] + 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5915b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "@tool(\"semantic_scholar_search\")\n",
    "def semantic_scholar_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch important academic papers related to a query.\n",
    "    This tool is stateless and does NOT update agent state.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---SEMANTIC SCHOLAR TOOL---\")\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "List important academic papers related to:\n",
    "\n",
    "{query}\n",
    "\n",
    "Return a concise list.\n",
    "\"\"\")\n",
    "    ]\n",
    "\n",
    "    model = ChatGroq(model=\"qwen-qwq-32b\")\n",
    "    response = model.invoke(msg)\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0520550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def check_relevance(state):\n",
    "    \"\"\"\n",
    "    Judge relevance of retrieved information.\n",
    "    \"\"\"\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    results = state.get(\"retrieved_docs\", [])\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\n",
    "Evaluate the relevance (0 to 1) of the following retrieved information:\n",
    "\n",
    "{results}\n",
    "\n",
    "Return ONLY a floating-point number between 0 and 1.\n",
    "\"\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model = ChatOpenAI(\n",
    "        model=\"deepseek-chat\",\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        base_url=\"https://api.deepseek.com\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    response = model.invoke(msg)\n",
    "\n",
    "    try:\n",
    "        score = float(response.content.strip())\n",
    "    except ValueError:\n",
    "        score = 0.0  # safety fallback\n",
    "\n",
    "    return {\"relevance_score\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24b5de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_router(state):\n",
    "    \"\"\"\n",
    "    Decide whether to refine or generate response.\n",
    "    Max refinement iterations = 2.\n",
    "    \"\"\"\n",
    "    score = state.get(\"relevance_score\", 0.0)\n",
    "    iteration = state.get(\"iteration_count\", 0)\n",
    "\n",
    "    if score >= 0.6:\n",
    "        return \"generate\"\n",
    "\n",
    "    if iteration >= 2:\n",
    "        return \"generate\"\n",
    "\n",
    "    return \"refine\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "041c2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def refine_query(state):\n",
    "    \"\"\"\n",
    "    Rewrite the query if relevance is low.\n",
    "    \"\"\"\n",
    "    print(\"---REFINE QUERY---\")\n",
    "\n",
    "    question = state[\"messages\"][0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\n",
    "Rewrite the following query to be more specific for faculty research matching.\n",
    "Do NOT add explanations.\n",
    "Return ONLY the rewritten query.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\"\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model = ChatOpenAI(\n",
    "        model=\"deepseek-chat\",\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        base_url=\"https://api.deepseek.com\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    response = model.invoke(msg)\n",
    "\n",
    "    refined_query = response.content.strip()\n",
    "\n",
    "    # Return PARTIAL update (no state mutation)\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=refined_query)],\n",
    "        \"iteration_count\": state[\"iteration_count\"] + 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ec6db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(state):\n",
    "    \"\"\"\n",
    "    Generate final answer for the user.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE RESPONSE---\")\n",
    "    results = state.get(\"search_results\", {})\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Based on the following research data, generate a concise faculty recommendation:\n",
    "\n",
    "{results}\n",
    "\"\"\")\n",
    "    ]\n",
    "\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=os.getenv(\"GEMINI\"),\n",
    "        temperature=0.3\n",
    "    )\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d181912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory(state):\n",
    "    \"\"\"\n",
    "    Store the latest interaction in memory.\n",
    "    \"\"\"\n",
    "    print(\"---UPDATE MEMORY---\")\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    memory = state.get(\"memory\", [])\n",
    "\n",
    "    # Extract last user question and assistant answer\n",
    "    user_msg = None\n",
    "    assistant_msg = None\n",
    "\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, AIMessage) and assistant_msg is None:\n",
    "            assistant_msg = msg.content\n",
    "        elif isinstance(msg, HumanMessage) and user_msg is None:\n",
    "            user_msg = msg.content\n",
    "        if user_msg and assistant_msg:\n",
    "            break\n",
    "\n",
    "    # If incomplete, do nothing\n",
    "    if not user_msg or not assistant_msg:\n",
    "        return {}\n",
    "\n",
    "    return {\n",
    "        \"memory\": memory + [\n",
    "            {\n",
    "                \"question\": user_msg,\n",
    "                \"answer\": assistant_msg\n",
    "            }\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c2588dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(ResearchAgentState)\n",
    "\n",
    "# ===== Nodes =====\n",
    "graph.add_node(\"query_analyzer\", query_analyzer)\n",
    "graph.add_node(\"memory_check\", memory_check)\n",
    "graph.add_node(\"intent_classifier\", intent_classifier)\n",
    "\n",
    "graph.add_node(\"retrieve_with_flashrank\", retrieve_with_flashrank)\n",
    "\n",
    "# External knowledge (non-persistent)\n",
    "graph.add_node(\"wikipedia\", wikipedia_node)\n",
    "\n",
    "graph.add_node(\"check_relevance\", check_relevance)\n",
    "graph.add_node(\"refine_query\", refine_query)\n",
    "\n",
    "graph.add_node(\"generate_response\", generate_response)\n",
    "graph.add_node(\"update_memory\", update_memory)\n",
    "\n",
    "# ===== Edges =====\n",
    "\n",
    "# START â†’ Analyzer â†’ Memory â†’ Intent\n",
    "graph.set_entry_point(\"query_analyzer\")\n",
    "graph.add_edge(\"query_analyzer\", \"memory_check\")\n",
    "graph.add_edge(\"memory_check\", \"intent_classifier\")\n",
    "\n",
    "# Intent branching\n",
    "graph.add_conditional_edges(\n",
    "    \"intent_classifier\",\n",
    "    intent_router,\n",
    "    {\n",
    "        \"new_search\": \"retrieve_with_flashrank\",\n",
    "        \"refinement\": \"retrieve_with_flashrank\",\n",
    "        \"follow_up\": \"retrieve_with_flashrank\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Retrieval â†’ External context â†’ Relevance\n",
    "graph.add_edge(\"retrieve_with_flashrank\", \"wikipedia\")\n",
    "graph.add_edge(\"wikipedia\", \"check_relevance\")\n",
    "\n",
    "# Relevance decision\n",
    "graph.add_conditional_edges(\n",
    "    \"check_relevance\",\n",
    "    relevance_router,\n",
    "    {\n",
    "        \"generate\": \"generate_response\",\n",
    "        \"refine\": \"refine_query\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Refinement loop (max 2 enforced)\n",
    "graph.add_edge(\"refine_query\", \"retrieve_with_flashrank\")\n",
    "\n",
    "# Finalization\n",
    "graph.add_edge(\"generate_response\", \"update_memory\")\n",
    "graph.add_edge(\"update_memory\", END)\n",
    "\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71b790aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"faculty working on computer vision at IIIT Naya Raipur\")],\n",
    "    \"intent\": \"\",\n",
    "    \"memory\": [],\n",
    "    \"retrieved_docs\": [],\n",
    "    \"relevance_score\": 0.0,\n",
    "    \"iteration_count\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bd65a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ANALYZE QUERY---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_state = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3050\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3048\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3050\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3064\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2633\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2631\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2632\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2643\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mquery_analyzer\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      8\u001b[39m     msg = [\n\u001b[32m      9\u001b[39m         HumanMessage(content=\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33mAnalyze the following query and clean it if needed.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     16\u001b[39m     ]\n\u001b[32m     19\u001b[39m     model = ChatGoogleGenerativeAI(\n\u001b[32m     20\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgemini-1.5-flash\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m         google_api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mGEMINI\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     22\u001b[39m         temperature=\u001b[32m0\u001b[39m\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response],\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33miteration_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m\n\u001b[32m     32\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1676\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1673\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1674\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1676\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1225\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1223\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1229\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1790\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   1789\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.max_retries\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:238\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    231\u001b[39m params = (\n\u001b[32m    232\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    237\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\tenacity\\__init__.py:487\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[32m    486\u001b[39m     retry_state.prepare_for_next_attempt()\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AIAGENTS\\venv\\Lib\\site-packages\\tenacity\\nap.py:31\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "final_state = app.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "for msg in reversed(final_state[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage):\n",
    "        print(msg.content)\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
